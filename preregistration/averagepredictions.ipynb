{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98772f22-efe6-4036-9f5b-7fff4bfbbe92",
   "metadata": {},
   "source": [
    "# Average the predictions\n",
    "\n",
    "Researchers provided predictions about the social variables that would correlate with novelty and predict precocity. Here we average them and estimate variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89980574-e052-4892-be24-927570f954b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b6065-3b8e-4526-a13c-2359e9d7c236",
   "metadata": {},
   "source": [
    "### The social variables we're talking about\n",
    "\n",
    "I've provided the complete list, but the ones that really interest us below are in bold.\n",
    "\n",
    "1. Books that got the most reviews.\n",
    "2. **Books about which most was written (adding up the length of each review).**\n",
    "3. **Books more positively reviewed.**\n",
    "4. Books reviewed by specific publications (we can itemize, say, 10 leading publications in the BRD).\n",
    "5. Books widely reviewed by little magazines; this is one way of defining an \"avant garde.\"\n",
    "6. Books published by particularly prestigious publishers (e.g., Knopf).\n",
    "7. **Books that won Pulitzer/Nobel prizes.**\n",
    "8. **Bestsellers (the top 10 per year from Unsworth’s list).**\n",
    "9. **We can use principal component analysis on the whole reception matrix, and then “rotate” the components to find one that tends to distinguish wide-circulation venues (like newspapers) from little magazines. This is another way of defining “avant garde,” and arguably better than the absolute count in (5) at identifying books that get relatively more attention in intellectual venues than in mass-market ones.**\n",
    "10. **A retrospective definition of the early-20c avant-garde extracted from recent 21c secondary sources by David Bishop and Liza Senatorova.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea723b56-8397-4ebb-a960-4a5dafc64e4e",
   "metadata": {},
   "source": [
    "### Predictions about novelty\n",
    "\n",
    "We each created a ranked list that predicted how well these variables would correlate with novelty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3219e3ba-2a0f-451b-b29c-b75131eab13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "novelty = dict()\n",
    "novelty['d'] = [9, 3, 2, 7, 8, 10]\n",
    "novelty ['l'] = [10, 9, 7, 2, 8, 3]\n",
    "novelty['t'] = [9, 10, 8, 2, 7, 3]\n",
    "novelty['w'] = [10, 9, 2, 7, 3, 8]\n",
    "novelty['y'] = [10, 9, 8, 7, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5321243e-c190-4763-b7bc-7adc23f88189",
   "metadata": {},
   "source": [
    "### Convert into a list of ranks\n",
    "\n",
    "We're given a ranked list of items. Since the item numbers have no meaning, we need to convert into a list of ranks before averaging.\n",
    "\n",
    "For each (real) item, we want to know \"what rank did it have in this list\"?\n",
    "\n",
    "Note that this is not the same thing as asking \"for each position in the list, what's the mapping to the arbitrary order 2, 3, 7, 8, 9, 10\"? Since the original ordering of those elements was arbitary, a number representing \"order in the original list\" doesn't have any substantive meaning.\n",
    "\n",
    "We will use that arbitrary order 2, 3, 7 etc to *organize* our lists (because they have to have some fixed order in common). But the numbers that we're interested in (and that we want to average) are the *rankings* provided by the five of us. Those are the numbers that have a substantive meaning (\"how much will this item correlate with novelty\") and that we therefore want to average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be0f1d91-f7e3-4c34-a061-191893ddec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankedlisttolistofranks(adict):\n",
    "    \n",
    "    ''' Converts a dictionary where each value is a\n",
    "    researcher-specific ordered list of six items\n",
    "    into a dictionary where each value is a list aligned with the\n",
    "    masterlist [2, 3, 7, 8, 9, 10]\n",
    "    and each element i of these new lists reports the rank of\n",
    "    masterlist[i] in the researcher's ordered list.'''\n",
    "    \n",
    "    #sanity check\n",
    "    assert len(adict) == 5\n",
    "    \n",
    "    masterlist = [2, 3, 7, 8, 9, 10]\n",
    "    \n",
    "    newdict = dict()\n",
    "    for k, v in adict.items():\n",
    "        assert len(v) == 6   # sanity check  \n",
    "        ranklist = []\n",
    "        for item in masterlist:\n",
    "            ranklist.append(v.index(item))\n",
    "        newdict[k] = ranklist\n",
    "    return newdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2314e293-4dda-4d00-b253-15ea48ed5916",
   "metadata": {},
   "source": [
    "Using this function we can convert our ranked lists into lists of ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6092b57d-d3f4-4f68-814a-272bad1b1eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d': [2, 1, 3, 4, 0, 5],\n",
       " 'l': [3, 5, 2, 4, 1, 0],\n",
       " 't': [3, 5, 4, 2, 0, 1],\n",
       " 'w': [2, 4, 3, 5, 1, 0],\n",
       " 'y': [4, 5, 3, 2, 1, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noveltyranks = rankedlisttolistofranks(novelty)\n",
    "noveltyranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30f3274-eca9-40e0-b800-c704fad0c8e6",
   "metadata": {},
   "source": [
    "### A function to average the ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f190ea97-5a64-4dd6-bcb1-5ed84642949b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.8, 4. , 3. , 3.4, 0.6, 1.2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def averagealltheranks(adict):\n",
    "    averageranks = np.zeros(6)\n",
    "    for k, v in adict.items():\n",
    "        averageranks = averageranks + v\n",
    "    averageranks = averageranks / len(adict)\n",
    "    return averageranks\n",
    "    \n",
    "averagenovranks = averagealltheranks(noveltyranks)\n",
    "averagenovranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d18ee5-4da5-4e89-847b-c0a00fceb3f6",
   "metadata": {},
   "source": [
    "That's our list of average ranks. Now we can use this to infer a ranking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f266fa-6c60-49d4-9fe6-b797a444e76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 10, 2, 7, 8, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuplelist = zip(averagenovranks, [2, 3, 7, 8, 9, 10])\n",
    "avgnovorder = [x[1] for x in sorted(tuplelist)]\n",
    "avgnovorder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93872ad9-d68c-4a23-8f26-aca32987f2e7",
   "metadata": {},
   "source": [
    "### Calculating Kendall's W\n",
    "\n",
    "[Kendall's W](https://en.wikipedia.org/wiki/Kendall%27s_W) is a measure of agreement between different rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5e59f06-c2b0-4fd8-9c3c-ee07a0f7be00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated value of W: 0.525  Permutation values exceed it in 67 out of 1000 cases\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def kendall_w(expt_ratings):\n",
    "    if expt_ratings.ndim!=2:\n",
    "        raise 'ratings matrix must be 2-dimensional'\n",
    "    m = expt_ratings.shape[0] #raters\n",
    "    n = expt_ratings.shape[1] # items rated\n",
    "    denom = m**2*(n**3-n)\n",
    "    rating_sums = np.sum(expt_ratings, axis=0)\n",
    "    S = n*np.var(rating_sums)\n",
    "\n",
    "    return 12*S/denom\n",
    "\n",
    "the_ratings = np.array([[1,2,3,4],[2,3,1,4],[1,3,2,4],[1,3,4,2]])\n",
    "m = the_ratings.shape[0]\n",
    "n = the_ratings.shape[1]\n",
    "\n",
    "W = kendall_w(the_ratings)\n",
    "\n",
    "count = 0\n",
    "for trial in range(1000):\n",
    "    perm_trial = []\n",
    "    for _ in range(m):\n",
    "        perm_trial.append(list(np.random.permutation(range(1, 1+n))))\n",
    "    count += 1 if kendall_w(np.array(perm_trial)) > W else 0\n",
    "\n",
    "print ('Calculated value of W:', W, ' Permutation values exceed it in', count, 'out of 1000 cases')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aae664-b656-4a57-83cc-46f347d25424",
   "metadata": {},
   "source": [
    "That gives us both a measure of *W* and something analogous to a p-value, telling us how often this could happen by accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cb9f050-56c4-47b4-a546-75cdf784d432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.067"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edf08006-e476-4373-9e92-40ce3ff70c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated value of W: 0.49714285714285716  Permutation values exceed it in 11 out of 1000 cases\n",
      "raters:  5  items rated:  6 and W =  0.49714286\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "novelty_array = np.array([x for x in noveltyranks.values()])\n",
    "\n",
    "m = novelty_array.shape[0]\n",
    "n = novelty_array.shape[1]\n",
    "\n",
    "W = kendall_w(novelty_array)\n",
    "\n",
    "count = 0\n",
    "for trial in range(1000):\n",
    "    perm_trial = []\n",
    "    for _ in range(m):\n",
    "        perm_trial.append(list(np.random.permutation(range(1, 1+n))))\n",
    "    count += 1 if kendall_w(np.array(perm_trial)) > W else 0\n",
    "\n",
    "print ('Calculated value of W:', W, ' Permutation values exceed it in', count, 'out of 1000 cases')\n",
    "print (\"raters: \",m,\" items rated: \", n, \"and W = \",'%.8f' % W)\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93667da3-ee6e-453b-93bc-636db7b6fd56",
   "metadata": {},
   "source": [
    "### How much the typical prediction diverges from the average\n",
    "\n",
    "A simpler and more familiar way to talk about variance may be to measure the average Spearman correlation of individual rankings with the center of gravity of them all.\n",
    "\n",
    "In an earlier version I suggested measuring this pairwise. We could still do that, but this measure seemed more comparable to the one we're going to get after the experiment: Spearman correlation of a single set of measurements with our predicted average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "080c6723-8de0-4d55-9beb-68ecf7a3fc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average Spearman rho is 0.7589548509215696\n",
      "Note not the same as 0.68\n",
      "The range of rhos is  [0.08571428571428573, 0.8857142857142858, 0.8285714285714287, 0.8857142857142858, 0.7142857142857143]\n"
     ]
    }
   ],
   "source": [
    "allzs = []\n",
    "allrhos = []\n",
    "\n",
    "for key, ranking in noveltyranks.items():\n",
    "    r, p = spearmanr(ranking, averagenovranks)\n",
    "    z = np.arctanh(r)\n",
    "    allzs.append(z)\n",
    "    allrhos.append(r)\n",
    "\n",
    "print('The average Spearman rho is', np.tanh(np.mean(allzs)))\n",
    "print('Note not the same as', np.mean(allrhos))\n",
    "print('The range of rhos is ', allrhos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af94212b-60a9-4378-9dfd-66014fe73fda",
   "metadata": {},
   "source": [
    "### Predictions about precocity\n",
    "\n",
    "Now we'll do the same thing for precocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a77ffb9c-ba31-4402-86b6-589dc5060311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d': [2, 5, 1, 0, 4, 3],\n",
       " 'l': [3, 5, 0, 2, 4, 1],\n",
       " 't': [0, 5, 3, 2, 4, 1],\n",
       " 'w': [3, 2, 1, 4, 5, 0],\n",
       " 'y': [4, 5, 3, 1, 0, 2]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precocity = dict()\n",
    "precocity['d'] = [8, 7, 2, 10, 9, 3]\n",
    "precocity['l'] = [7, 10, 8, 2, 9, 3]\n",
    "precocity['t'] = [2, 10, 8, 7, 9, 3]\n",
    "precocity['w'] = [10, 7, 3, 2, 8, 9]\n",
    "precocity['y'] = [9, 8, 10, 7, 2, 3]\n",
    "\n",
    "precocityranks = rankedlisttolistofranks(precocity)\n",
    "precocityranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "391b832f-22fc-41a1-9d0f-c39da6b55c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.4, 4.4, 1.6, 1.8, 3.4, 1.4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averageprecranks = averagealltheranks(precocityranks)\n",
    "averageprecranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a59df993-a40a-4fd6-88c9-16fca6feef20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18789425188745115, 0.7214753549798081)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(averagenovranks, averageprecranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f20b356-01da-46b5-93bf-ce0c6c631d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 7, 8, 2, 9, 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuplelist = zip(averageprecranks, [2, 3, 7, 8, 9, 10])\n",
    "avgprecorder = [x[1] for x in sorted(tuplelist)]\n",
    "avgprecorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51d49bb9-dc65-4c0c-954c-26af14595e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated value of W: 0.3965714285714286  Permutation values exceed it in 51 out of 1000 cases\n",
      "6 5\n"
     ]
    }
   ],
   "source": [
    "precocity_array = np.array([x for x in precocityranks.values()])\n",
    "\n",
    "m = precocity_array.shape[0]\n",
    "n = precocity_array.shape[1]\n",
    "\n",
    "W = kendall_w(precocity_array)\n",
    "\n",
    "count = 0\n",
    "for trial in range(1000):\n",
    "    perm_trial = []\n",
    "    for _ in range(m):\n",
    "        perm_trial.append(list(np.random.permutation(range(1, 1+n))))\n",
    "    count += 1 if kendall_w(np.array(perm_trial)) > W else 0\n",
    "\n",
    "print ('Calculated value of W:', W, ' Permutation values exceed it in', count, 'out of 1000 cases')\n",
    "print(n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4778b540-1074-4aa4-a7bb-e9e377352a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average Spearman rho is 0.6756864375860854\n",
      "Note not the same as 0.6\n",
      "The range of rhos is  [0.6, 0.942857142857143, 0.6, 0.6, 0.2571428571428572]\n"
     ]
    }
   ],
   "source": [
    "allzs = []\n",
    "allrhos = []\n",
    "\n",
    "for key, ranking in precocityranks.items():\n",
    "    r, p = spearmanr(ranking, averageprecranks)\n",
    "    z = np.arctanh(r)\n",
    "    allzs.append(z)\n",
    "    allrhos.append(r)\n",
    "\n",
    "print('The average Spearman rho is', np.tanh(np.mean(allzs)))\n",
    "print('Note not the same as', np.mean(allrhos))\n",
    "print('The range of rhos is ', allrhos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5addf66d-34e4-4170-9806-ed8732dd7661",
   "metadata": {},
   "source": [
    "### Relation of novelty predictions to precocity predictions\n",
    "\n",
    "Finally, how do the two rankings agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23df19b2-8090-4bf5-b1ab-8a8f62fc338d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.3142857142857143, pvalue=0.5440932944606414)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(avgprecorder, avgnovorder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
